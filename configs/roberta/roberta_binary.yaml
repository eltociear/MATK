# lightning.pytorch==2.0.0
seed_everything: 1111
trainer:
  accelerator: gpu
  devices: 2
  strategy: ddp_find_unused_parameters_true
  max_epochs: 1
  accumulate_grad_batches: 1
  enable_checkpointing: True
  limit_train_batches: 1
  callbacks: 
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: checkpoints/roberta/fhm
        monitor: val_auroc
        mode: max
        save_top_k: 1
        every_n_epochs: 1
        save_last: True
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val_auroc
        patience: 5
        mode: max
model:
  class_path: models.roberta.RobertaClassificationModel
  init_args:
    model_class_or_path: roberta-base
    num_classes: 2
data:
  class_path: datamodules.fhm.FHMDataModule
  init_args:
    dataset_class: FHMDataset
    img_dir: /mnt/sdb/aditi/hateful_memes/hateful_memes/
    annotation_filepaths: {
      train: /mnt/sdb/aditi/hateful_memes/hateful_memes/train.jsonl,
      validate: /mnt/sdb/aditi/hateful_memes/hateful_memes/dev_seen.jsonl,
      test: /mnt/sdb/aditi/hateful_memes/hateful_memes/dev_seen.jsonl,
      predict: /mnt/sdb/aditi/hateful_memes/hateful_memes/dev_seen.jsonl
    }
    model_class_or_path: roberta-base
    shuffle_train: True
    batch_size: 32
