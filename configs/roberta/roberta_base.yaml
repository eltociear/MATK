# lightning.pytorch==2.0.0
seed_everything: 1111
trainer:
  accelerator: gpu
  devices: 2
  strategy: ddp_find_unused_parameters_true
  max_epochs: 1
  accumulate_grad_batches: 1
  enable_checkpointing: True
  limit_train_batches: 1
  callbacks: 
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: checkpoints/roberta/fhm
        monitor: val_auroc
        mode: max
        save_top_k: 1
        every_n_epochs: 1
        save_last: True
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val_auroc
        patience: 5
        mode: max
model:
  class_path: models.roberta_base_model.ActualModel
  init_args:
    model_class_or_path: roberta-large
    opt: 
      DATASET : harm
      FEW_SHOT : False
      FINE_GRIND : False
      NUM_SHOTS : 16
      MODEL : pbm
      UNIMODAL : False
      DATA : /mnt/sdb/aditi/multimodal-memes-toolkit/datamodules/data
      CAPTION_PATH : /mnt/sdb/aditi/multimodal-memes-toolkit/preprocessing/clipcap
      RESULT : ./result
      FEAT_DIM : 2048
      CLIP_DIM : 512
      BERT_DIM : 768
      ROBERTA_DIM : 1024
      NUM_FOLD : 5
      EMB_DIM : 300
      NUM_LABELS : 2
      POS_WORD : good
      NEG_WORD : bad
      DEM_SAMP : False
      SIM_RATE : 0.5
      IMG_RATE : 0.5
      TEXT_RATE : 0.5
      CLIP_CLEAN : False
      MULTI_QUERY : True
      NUM_QUERIES : 4
      EMB_DROPOUT : 0.0
      FC_DROPOUT : 0.4
      WEIGHT_DECAY : 0.01
      LR_RATE : 1e-05
      EPS : 1e-08
      BATCH_SIZE : 16
      FIX_LAYERS : 0
      MID_DIM : 512
      NUM_HIDDEN : 512
      LENGTH : 64
      TOTAL_LENGTH : 256
      PREFIX_LENGTH : 10
      NUM_SAMPLE : 1
      NUM_LAYER : 8
      MODEL_NAME : roberta-large
      PRETRAIN_DATA : conceptual
      IMG_VERSION : clean
      MAPPING_TYPE : transformer
      ADD_ENT : True
      ADD_DEM : True
      DEBUG : False
      SAVE : False
      SAVE_NUM : 1
      SEED : 1111
      WARM_UP : 2000
      TRANS_LAYER : 1
      NUM_HEAD : 8
data:
  class_path: datamodules.prompthate_data.MultimodalDataModule
  init_args:
    model_class_or_path: roberta-large
    shuffle_train: True
    batch_size: 1
    opt: 
      DATASET : harm
      FEW_SHOT : False
      FINE_GRIND : False
      NUM_SHOTS : 16
      MODEL : pbm
      UNIMODAL : False
      DATA : /mnt/sdb/aditi/multimodal-memes-toolkit/datamodules/data
      CAPTION_PATH : /mnt/sdb/aditi/multimodal-memes-toolkit/preprocessing/clipcap
      RESULT : ./result
      FEAT_DIM : 2048
      CLIP_DIM : 512
      BERT_DIM : 768
      ROBERTA_DIM : 1024
      NUM_FOLD : 5
      EMB_DIM : 300
      NUM_LABELS : 2
      POS_WORD : good
      NEG_WORD : bad
      DEM_SAMP : False
      SIM_RATE : 0.5
      IMG_RATE : 0.5
      TEXT_RATE : 0.5
      CLIP_CLEAN : False
      MULTI_QUERY : True
      NUM_QUERIES : 4
      EMB_DROPOUT : 0.0
      FC_DROPOUT : 0.4
      WEIGHT_DECAY : 0.01
      LR_RATE : 1e-05
      EPS : 1e-08
      BATCH_SIZE : 16
      FIX_LAYERS : 0
      MID_DIM : 512
      NUM_HIDDEN : 512
      LENGTH : 64
      TOTAL_LENGTH : 256
      PREFIX_LENGTH : 10
      NUM_SAMPLE : 1
      NUM_LAYER : 8
      MODEL_NAME : roberta-large
      PRETRAIN_DATA : conceptual
      IMG_VERSION : clean
      MAPPING_TYPE : transformer
      ADD_ENT : True
      ADD_DEM : True
      DEBUG : False
      SAVE : False
      SAVE_NUM : 1
      SEED : 1111
      WARM_UP : 2000
      TRANS_LAYER : 1
      NUM_HEAD : 8
